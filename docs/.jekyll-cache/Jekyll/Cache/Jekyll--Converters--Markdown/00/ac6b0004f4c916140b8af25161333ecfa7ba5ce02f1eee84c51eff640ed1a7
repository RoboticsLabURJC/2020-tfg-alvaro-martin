I"Ð<h2 id="new-data">New data</h2>
<p>Since we have obtained a good result with the frames that include a linear motion we proceed to increase a degree of freedom in this type of data letting the point start at a random height. In the following <a href="https://roboticslaburjc.github.io/2017-tfm-nuria-oyaga/previous%20work/datasets/">link</a> you can find a description of this new type of images.</p>

<h2 id="non-recurrent-neural-networks">Non Recurrent Neural Networks</h2>
<p>I used the same structure that in the previous training</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Non-Recurrent/Frame_point_linear/5000_samples_y0/15_False_relu_categorical_crossentropy_10_properties.png" alt="2D Convolutional network structure" />
  </figure>

<p>As I mentioned earlier, with this new degree of freedom the complexity of the problem increases and, as expected, the performance of the network is not good.</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Non-Recurrent/Frame_point_linear/5000_samples_y0/15_False_relu_categorical_crossentropy_10_history.png" alt="Loss history" />
  </figure>

<figure class="">
  <img src="/assets/images/logbook/media/Models/Non-Recurrent/Frame_point_linear/5000_samples_y0/15_False_relu_categorical_crossentropy_10_error_hist.png" alt="Error histogram" />
  </figure>

<figure class="">
  <img src="/assets/images/logbook/media/Models/Non-Recurrent/Frame_point_linear/5000_samples_y0/15_False_relu_categorical_crossentropy_10_rel_error_hist.png" alt="Relative error histogram" />
  </figure>

<p>In the next image you can see target frame in the samples where the errors (absolute and relative) are maximum. Consequently, the error made is very high.</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Non-Recurrent/Frame_point_linear/5000_samples_y0/15_False_relu_categorical_crossentropy_10_max_error.png" alt="Relative and absolute error" />
  </figure>

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>
<p>I used the same structure that in the previous training:</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Recurrent/Frames/linear_point_255_y0/15_False_relu_categorical_crossentropy_10_properties.png" alt="ConvLSTM network structure" />
  </figure>

<p>In this case we obtain a result that we could consider as expected, these networks are able to better capture the temporal relationship and obtain a better performance than in the previous case.</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Recurrent/Frames/linear_point_255_y0/15_False_relu_categorical_crossentropy_10_history.png" alt="Loss history" />
  </figure>

<figure class="">
  <img src="/assets/images/logbook/media/Models/Recurrent/Frames/linear_point_255_y0/15_False_relu_categorical_crossentropy_10_error_hist.png" alt="Error histogram" />
  </figure>

<figure class="">
  <img src="/assets/images/logbook/media/Models/Recurrent/Frames/linear_point_255_y0/15_False_relu_categorical_crossentropy_10_rel_error_hist.png" alt="Relative error histogram" />
  </figure>

<p>In the next image you can see target frame in the samples where the errors (absolute and relative) are maximum. As I mentioned, this type of structure has improved the results and the error made, despite remaining high, is reduced compared to the previous structure.</p>
<figure class="">
  <img src="/assets/images/logbook/media/Models/Recurrent/Frames/linear_point_255_y0/15_False_relu_categorical_crossentropy_10_max_error.png" alt="Relative and absolute error" />
  </figure>

<p>In order to improve the obtained results we will choose to increase the number of samples in consequence with the complexity of the problem and modify the structure of the network in the same way.</p>
:ET